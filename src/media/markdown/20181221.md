---
title: '📖 Thinking Fast And Slow'
date: '2018-12-21'
path: '/blog/2018/thinking-fast-and-slow'
tags: ['books', 'economics', 'psychology']
featured: 'false'
excerpt: ''
source: 'https://www.amazon.com/dp/0374533555/'
---

## Introduction

🖇 Related to [Free Will](/blog/2018/free-will) most impressions and thoughts arise in your conscious experience without your knowing how they got there.

🖇 Related to [Moneyball](https://www.amazon.es/dp/B000RH0C8G/) we are far too willing to believe research findings based on inadequate evidence and prone to collect too few observations in our own research.

We use resemblance as a simplifying heuristic — a rule of thumb — to make a difficult judgement. When the question is difficult and a skilled solution is not available, intuition still has a shot, but we often answer an easier question instead, without noticing the substitution.

🎲 Good examples from the book are: Steve the librarian, the number of times _K_ being more likely to appear in the first letter of a word or why to pick Ford stocks.

People tend to assess the relative importance of issues by the ease with which they are retrieved from memory — which is largely determined by the extent of coverage in the media.

## Part 1 — Two Systems

- System 1: the instant, no effort, unconscious, no sense of voluntary control, automatic, emotional, intuitive thinking. It has biases and has little understanding of logic and statistics.
- System 2: the slower, conscious, rational, reasoning, deliberate thinking.

System 2 is normally comfortable in "low-effort" mode, System 1 continuously generates suggestions for System 2. If endorsed by System 2, intuitions, intentions, and feelings, turn into beliefs. When everything goes smoothly, System 2 adopts the suggestions of System 1.

System 2 has the capability to adopt a set of "task sets", it can program memory to obey an instruction and override habitual responses.

🎲 [The Invisible Gorilla - Wikipedia](https://en.wikipedia.org/wiki/The_Invisible_Gorilla) by Christopher Chabris and Daniel Simons.

> We can be blind to the obvious, and we are also blind to blindness.

🖇 Related to Rumsfel's [There are known knowns - Wikipedia](https://en.wikipedia.org/wiki/There_are_known_knowns)

📍 Ego depletion

We dispose a limited budget of attention to allocate to activities — we can't go beyond this budget. Imagine System 2 as the electrical circuit on your home, but they respond different to overload. Whilst a breaker trips on the electrical circuit, shutting all devices down, System 2 is selective and protects the "most important" activity — hence the Gorilla experiment.

🎲 [Müller-Lyer illusion - Wikipedia](https://en.wikipedia.org/wiki/M%C3%BCller-Lyer_illusion)

As we become skilled in a task, its demand for "energy" diminishes. The acquisition of skill is driven by the balance of benefits and costs: laziness is built deep into our nature.

Even self-control requires discipline in contrast of flow — a state of effortless concentration so deep that we lose sense of ourselves. [States of flow](<https://en.wikipedia.org/wiki/Flow_(psychology)>) "shut down" the self-control, freeing up resources to be directed to the task at hand.

Self-control and cognitive effort are forms of mental work.

🎲 [Roy Baumeister - Wikipedia](https://en.wikipedia.org/wiki/Roy_Baumeister) has demonstrated that all the variants of voluntary effort — cognitive, emotional, physical or activities that impose high demands on System 2 — draw at least partly on a shared pool of mental effort — aka. ego depletion.

The nervous system is the primer consumer of glucose hence effortful mental activities are expensive in the currency of glucose. The implication of this idea is that the effects of ego depletion can be undone by consuming glucose.

🖇 Related to [Keith Stanovich — Rationality and the Reflective Mind]( Keith Stanovich ) argues that rationality should be distinguished from intelligence. The later he calls it algorithmic, and deals with slow thinking, but higher scores of intelligence do not come free of biases. Superficial or "lazy thinking" is a flaw in the reflective mind, a failure of rationality.

We are associative machines. An idea that has been activated evokes many other ideas, not in a series, but simultaneously, which in turn activate many others. Again related to [Free Will](/blog/2018/free-will), only a few will register in consciousness, most of the work of associative thinking is silent, hidden from our conscious self.

> We know far less about ourselves than we feel we do.

📍 [Priming - Wikipedia](https://en.wikipedia.org/wiki/Priming) effect, not just states of mind prime behavior or decisions, but also actions and emotions can be primed by events of which we are not even aware.

🎲 John Bargh and the "Florida effect" involves two stages of priming. First, the set of words primes thoughts of old age, though the word _old_ is never mentioned; second, these thoughts prime behavior, walking slowly, which is associative with old age.

📍 Cognitive ease: ranges between "Easy" i.e. the situation is comfortably familiar, mostly directed by System 1; to "Strained" in which a problem exists and the mobilization of System 2 is required. When strained, you're likely to be more vigilant and suspicious, but also less creative.

Cognitive ease can be triggered by a repeated experience — familiarity is not easily distinguished from truth, a clearer font, a primed idea or good mood. In addition to make simpler messages or make them memorable i.e. verses are more likely to be taken as true. Hence cognitive ease has multiple causes, but it is really difficult to tease them apart.

🎲 Robert Zajonc's [mere exposure effect - Wikipedia](https://en.wikipedia.org/wiki/Mere-exposure_effect) does not depend on the conscious experience or familiarity. It also appears when words are shown so quickly that the observers never became aware of having seen them. System 1 can respond to impressions of events that System 2 is unaware.

The mere exposure effect is linked to safety and survival, it occurs because the repeated exposure of a stimulus is followed by nothing bad.

🎲 Sarnoff Mednick's [Remote Associates Test - Wikipedia](https://en.wikipedia.org/wiki/Remote_Associates_Test)

- People's guesses are much accurate than they would be by chance. A sense of cognitive ease is apparently generated by a very faint signal from the associative machine.
- Unhappy subjects were unable to perform the intuitive task accurately. When we are uncomfortable and unhappy, we lose touch with intuition. \* Good mood, intuition, creativity and increased reliance on System 1 form a cluster.
- Most important, the impression of cognitive ease, that comes with the presentation of a coherent triad appears to be pleasurable in itself.

📍Causation

Finding causal connections is part of understanding a story and is an automatic operation of System 1. It is also "motivated" to jump into conclusions, it makes choices even without us noticing, which involves the construction of the best possible interpretation of a given situation.

> When the question is difficult and a skilled solution is not available, intuition still has a shot: an answer may come to mind quickly — but it is not an answer to the original question. When faced with a difficult question, we often answer an easier one instead, usually without noticing the substitution.

System 1 by default will first try to "make sense" or find a causal explanation to any event. Only System 2 is able to "unbelieve", thus subjects engaged with cognitive demanding tasks, tired or depleted will be more easily influenced.

If a satisfactory answer to a hard question is not found quickly, System 1 will find a related question that is easier and will answer it.

📍 Confirmation Bias

The operations of associative memory contribute to a general [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias). A deliberate search for confirming evidence, is how System 2 tests hypothesis. Contrary to the rules of philosophers in science, who test hypothesis by trying to refute them, people seek data that are likely to be compatible with the beliefs they currently hold.

The tendency to like or dislike everything about a person — including things you have not observed — is known as the [halo effect](https://en.wikipedia.org/wiki/Halo_effect). It also manifests when a handsome and confident speaker bounds onto the stage, you can expect the audience will judge his comments more favorably than he deserves.

📍WYSIATI

What you see is all there is — jumping into conclusions on the basis of limited evidence. The combination of a coherence-seeking System 1 (even when there's not enough information to make a decision) with a lazy System 2, implies that System 2 will endorse many intuitive beliefs, which closely reflect the impressions generated by System 1.

When information is scarce, which is a common occurrence, System 1 operates as a machine for jumping to conclusions. You did not start by asking, "What would I need to know before I formed an opinion about the quality of someone's leadership?" System 1 got to work on its own from the first adjective.

> Poor evidence can make a very good story.

You cannot help dealing with the limited information you have as if it were all there is to know. You build the best possible story from the information available to you, and if it is a good story, you believe it. Paradoxically, it is easier to construct a coherent story when you know little, when there are fewer pieces to fit into the puzzle. Our comforting conviction that the world makes sense rests on a secure foundation: our almost unlimited ability to ignore our ignorance.

📍 Intensity matching & substitution

Other features of System 1 are basic assessments — for example, our ability to judge people's competence by combining the two — unrelated — dimensions of strength and trustworthiness. System 1 deals readily answers questions that refer to an underlying dimension of intensity or amount — which permits the use of the word _more_. This is known as intensity matching. The most surprising thing is that it is also able to translate _matching_ values across dimensions.

It all comes down to its ability to generate intuitive opinions on complex matters. If a satisfactory answer to a hard question — target question — is not quickly found, System 1 will find a related question that is easier — heuristic question — and it will answer it. We call this phenomena substitution.

> The present state of mind looms very large when people evaluate their happiness.

The automatic processes fo the mental shotgun and intensity matching often make available one or more answers to easier questions that could be mapped out onto the target question.

## Part 2 — Heuristics And Biases

System 1 is always looking to identify causal connections between events, even when there is no such connection — it is looking for a "reason" to explain things.

🔖 The associative machinery seeks causes. The difficulty we have with statistical regularities is that they call for a different approach. Instead of focusing on how the event at hand came to be, the statistical view relates it to what could have happened instead. Nothing in particular caused it to be what it is — chance selected it from among its alternatives. Our predilection for causal thinking exposes us to serious mistakes in evaluating the randomness of truly random events.

> People are prone to apply causal thinking inappropriately, to situations that require statistical reasoning.

A random event does not lend itself to explanation, but collections of random events do behave in a regular fashion.

🖋 You can predict the outcome of a repeated sampling from an urn just as confidently as you can predict what will happen if you hit an egg with a hammer. You cannot predict the every detail of how it'll shatter, but you can be sure of the general idea. Statistically speaking, there is no difference between smashing an egg and the drawing of marbles — the only difference is the satisfying sense of causation that we experience when thinking of a hammer hitting an egg is altogether absent when thinking about sampling.

📍 Artifacts are observations that are produced by some aspect of the research. Large samples are more precise than small samples, which is the same to say that extreme outcomes are more likely to be found in small samples. This explanation is not causal.

More on chance and randomness:

- Nothing in particular caused it to be what it is — chance selected it from among its alternatives.
- To the untrained eye, randomness appears as regularity or tendency to cluster.
- The hot hand is a massive and widespread cognitive illusion.

We pay more attention to the content of messages than to the information about their reliability. For example, when an unlikely event becomes the focus of attention, we will assign it much more weight than its probability deserves.

📍 Anchoring effect

Your actions and your emotions can be primed by events of which you are not even aware. When people consider a particular value for an unknown quantity before estimating that quantity. It might have a lot of influence in making the first move in a single-issue negotiation.

A reliable way to make people believe in falsehoods is frequent repetition, because familiarity is not easily distinguished from truth.

Words that were presented more frequently were rated much more favorably than the words that had been shown only once or twice.

If repeated exposure of a stimulus is followed by nothing bad, such a stimulus will eventually become a safety signal.

📍 Availability heuristic

The experience of familiarity has a simple but powerful quality of 'pastness' that seems to indicate that it is a direct reflection of prior experience. This quality of pastness is an illusion.

Substitutes one question for another: you wish to estimate the size of a category or the frequency of an event, but you report an impression of the ease with which instances come to mind. For example, because of the coincidence of two planes crashing last month, somebody might prefer to take the train. The risk itself hasn't change, but it is a clear illustration of availability bias.

🎲 [Norbert Schwarz - Wikipedia](https://en.wikipedia.org/wiki/Norbert_Schwarz) demonstrated that the ease with which judgments came to mind, was a more powerful enhancer of availability heuristic than the number of instances retreated. For example, people asked for six instances in which they were assertive, rated themselves more assertive than the ones asked for twelve. Since coming up with six is way easier.

> The world in our heads is not a precise replica of reality — our expectations about the frequency of events are distorted by the prevalence and emotional intensity of the messages to which we are exposed.

🎲 [Paul Slovic - Wikipedia](https://en.wikipedia.org/wiki/Paul_Slovic) along those lines Paul Slovic demonstrated that estimates of causes of death are warped by media coverage, yet the coverage itself is biased towards novelty and poignancy.

📍 Affect heuristic people make judgements and decisions by consulting their emotions: Do I like it? Do I hate it? Instead of asking the true question: What do I think about it?

📍 Availability cascade is the underlying principle behind fake news into which biases flow into policy. It goes by "all heuristics are equal, but availability is more equal than others". The importance of an idea is often judged by the fluency and emotional charge with which it comes to mind.

Similarly, "probability neglect" describes the idea of focusing on the "numerator" (the tragic story on the news) rather than the "denominator" (the chances of such event occurring).

🎲 **Tom W's** specialty and the sins or representativeness: participants were responding along the lines of similarity to the stereotype (easier question) rather than probability and base rates (a far more difficult one). The main idea here is that base rates are neglected whenever information of the specific instance is presented.

🖇 Related to [Moneyball](https://www.imdb.com/title/tt1210166/) and the inefficiency of this mode of prediction.

When you have doubts about the quality of the evidence: let your judgments of probability stay close to the base rate.

📍 Bayesian reasoning — ideas to keep in mind:

- Base rates matter even in the presence of evidence of the case at hand, thus anchor your judgement of the probability of an outcome on a plausible base rate.
- Intuitive impressions of the diagnostic are often exaggerated. The combination of WYSIATI and associative coherence tends us to make us believe in the stories we spin for ourselves.

🎲 Linda: "less is more" illustrates how people miss logical relations (in terms of Venn diagrams) when faced with judgements of likelihood. But it is a fact that adding detail (that specially triggers intuition) to scenarios makes them more persuasive but less likely to come true. Usually, in the absence of competing intuition, logic prevails.

`p(Linda is a teller) = p(Linda is a feminist teller) + p(Linda is non-feminist teller)`

🎲 Joint evaluation vs. single evaluation experiments of `sets of dishes`, `RGRR vs. GRGRRR` and finally `health survey on percentages vs. how many` do no more than emphasize the laziness of the System 2.

Causes trump statistics: the mind primes causal base rates (which are usually treated as information about the individual case) over statistical base rates (which are usually underweighted).

Causal base rates can be turned into stereotypes, and they are how we think of categories and act as statements about the group that are accepted as facts about every member.

People who are taught surprising statistical facts about human behavior may be impressed to the point of telling their friends about what they've heard, but this does not mean their understanding of the world has changed. There is a big gap between our thinking about statistics and our thinking about individual cases.

📍 Regression to the mean

Feedback and rewards for improved performance work better than punishment of mistakes — as an example, instructors of the Israeli Air Force. They associated causal interpretation to the inevitable fluctuation of a random process: [regression to the mean](https://en.wikipedia.org/wiki/Regression_toward_the_mean), which was due to natural fluctuations in the quality of performance.

```
success = talent + luch
great success = a little more talent + a lot of luck
```

The "Sports Illustrated jinx," the claim that an athlete whose picture appears on the cover of the magazine is doomed to perform poorly the following season. Overconfidence and the pressure of meeting high expectations are often offered as explanations. But there is a simpler account of the jinx: an athlete who gets to be on the cover of Sports Illustrated must have performed exceptionally well in the preceding season, probably with the assistance of a nudge from luck - and luck is fickle.

🔖 Discovered by [Sir Francis Galton](https://en.wikipedia.org/wiki/Francis_Galton) found out that regression inevitably occurs when correlation between two measures is less than perfect. It took him several years though to figure out that [correlation](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation) and regression are not two concepts, but different interpretations of the same concept.

🎲 If you treated a group of depressed children for some time with an energy drink, they would show a clinically significant improvement. It is also the case that depressed children who spend some time standing on their head or hug a cat for twenty minutes a day will also show improvement. Most readers of such headlines will automatically infer that the energy drink or the cat hugging caused an improvement, but this conclusion is completely unjustified.

Depressed children are an extreme group, they are more depressed than most other children — and extreme groups regress to the mean over time. The correlation between depression scores on successive occasions of testing is less than perfect, so there will be regression to the mean: depressed children will get somewhat better over time even if they hug no cats and drink no Red Bull.

In order to conclude that an energy drink — or any other treatment — is effective, you must compare a group of patients who receive this treatment to a "control group" that receives no treatment (or, better, receives a placebo). The control group is expected to improve by regression alone, and the aim of the experiment is to determine whether the treated patients improve more than regression can explain.

Correcting for bias:

- If you know nothing about the case at hand, stay with the baseline prediction.
- Aim for a prediction which should be between the baseline and your intuitive prediction.
  _ If no useful evidence, stay within the baseline prediction.
  _ If after critical review you remain extremely confident on your initial prediction, keep your initial prediction.
- Estimate and apply the correlation, and you will end up somewhere between the two poles.

## Part 3 — Overconfidence

📍 Narrative fallacy

Introduced by [Nassim Taleb](https://en.wikipedia.org/wiki/Nassim_Nicholas_Taleb) — describe how flawed stories of the past can shape our views of the world and expectations for the future. They arise inevitably from our continuous attempt to make sense of the world.

We look for stories that are simple, concrete, assign a larger role to talent stupidity and intentions, rather than luck.

> A compelling narrative fosters an illusion of inevitability.

There are many phenomenas here working at the same time. First, WYSIATI — you can't help but dealing with the limited information you have as if it were all there is to know.

It is easier to construct a compelling story when you know little, when there are fewer pieces to fit the puzzle. Yet if an unpredicted event occurs, we immediately adjust our view of the world to accommodate the surprise.

> In everyday language, we use the word know only when what was known turned out to be true and can be shown to be true.

We believe we understand the past, which implies that the future should also be knowable, but in fact we understand the past less than we believe we do.

The mind that makes up narratives of the past is a sense-making organ. When an unpredicted event occurs, we immediately adjust our view of the world to accommodate the surprise. We have an imperfect ability to reconstruct past states of knowledge. Once you adopt a new view of the world (or any part of it), you immediately lose much of your ability to recall what you used to believe before your mind changed.

Consumers have a hunger for a clear message about the determinants of success and failure in business, and they need stories that offer a sense of understanding, however illusory.

📍 Hindsight bias

[Also known as the knew-it-all-along effect](https://en.wikipedia.org/wiki/Hindsight_bias), is the inclination, after an event has occurred, to see the event as having been predictable, despite there having been little or no objective basis for predicting it.

It leads observers to assess the quality of a decision not by whether the process was sound, but by whether the outcome was good or bad.

Yet the basic message of business books is that good managerial practices can be identified and that good practices will be rewarded by good results. Both messages are overstated.

🖇 Related idea — [The Halo Effect](https://en.wikipedia.org/wiki/Halo_effect). Because of it, we get causal relationships backwards: we are prone to believe that firms fails because the CEO is too rigid, when the truth is that the CEO is too rigid because the firm fails.

> Inevitably, we are always looking for causal explanations in events where luck plays a major role.

The comparison of firms that have been more or less successful is to a significant extent a comparison between firms that have been more or less lucky. Knowing the importance of luck, you should be particularly suspicious when highly consistent patterns emerge from the comparison of successful and less successful firms. In the presence of randomness, regular patterns can only be mirages.

The average gap must shrink, because the original gap was due in good part to luck, which contributed both to the success of the top firms and to the lagging performance of the rest. We have already encountered this statistical fact of life: regression to the mean.

📍 The illusion of validity

The amount of evidence and its quality do not count for much, because poor evidence can make a very good story.

The illusion that one has understood the past feeds the further illusion that one can predict and control the future. These illusions are comforting. They reduce the anxiety that we would experience if we allowed ourselves to fully acknowledge the uncertainties of existence. We all have a need for the reassuring message that actions have appropriate consequences, and that success will reward wisdom and courage. Many business books are tailor-made to satisfy this need.

"When you sell a stock," I asked, "who buys it?" He answered with a wave in the vague direction of the window, indicating that he expected the buyer to be someone else very much like him. That was odd: What made one person buy and the other sell? What did the sellers think they knew that the buyers did not?

> A major industry appears to be built largely on an illusion of skill.

The buyers and sellers know that they have the same information; they exchange the stocks primarily because they have different opinions.

If all assets in a market are correctly priced, no one can expect either to gain or to lose by trading. Perfect prices leave no scope for cleverness, but they also protect fools from their own folly.

For the large majority of individual investors, taking a shower and doing nothing would have been a better policy than implementing the ideas that came to their minds.

Individual investors predictably flock to companies that draw their attention because they are in the news. Professional investors are more selective in responding to news. These findings provide some justification for the label of "smart money" that finance professionals apply to themselves.

A basic test of skill: persistent achievement. The diagnostic for the existence of any skill is the consistency of individual differences in achievement.

The illusion of skill is not only an individual aberration; it is deeply ingrained in the culture of the industry. Facts that challenge such basic assumptions - and thereby threaten people's livelihood and self-esteem - are simply not absorbed. The mind does not digest them.

Skill in evaluating the business prospects of a firm is not sufficient for successful stock trading, where the key question is whether the information about the firm is already incorporated in the price of its stock. Traders apparently lack the skill to answer this crucial question, but they appear to be ignorant of their ignorance.

🔖 Our tendency to construct and believe coherent narratives of the past makes it difficult for us to accept the limits of our forecasting abilities. Everything makes sense in hindsight, and we can't suppress the powerful intuition that what makes sense in hindsight today, was predictable yesterday.

The illusion that we understand the past fosters overconfidence in our ability to predict the future. Yet the idea that historical events are determined by luck is profoundly shocking, although it is demonstrably true.

📍 Intuitions vs. formulas

Unreliable judgements can't be valid predictors of anything.

Because you have little direct knowledge of what goes into the mind, you'll never know that you might have made a different judgement or reached a different under very slightly different circumstances. Formulas do not suffer from such problems.

The surprising success of equal-weighting schemes has an important practical implication: it is possible to develop useful algorithms without any prior statistical research.

An algorithm that is constructed in the back of an envelope is often good enough to compete with an optimally weighted formula, and certainly good enough to outdo expert judgement. This logic can be applied to many domains, from stock picking to medical treatments.

The story of a child dying because an algorithm made a mistake is more poignant than the same story occurring because of a human error. The difference in emotional intensity is readily translated to a moral preference.

📍 Acquiring skill

When do judgements reflect true expertise? When do they display an illusion of validity? There are two conditions for acquiring a skill:

- An environment that is sufficiently regular to be predictable: intuition can't be trusted in the absence of regularities in the environment. \* Only extremely regular environments might justify an intuition that goes against the base rates.
- An opportunity to learn these regularities through prolonged practice: we are looking here for quick feedback loops, if there is a huge delay between actions and their noticeable outcomes, its learning becomes harder. \* As an example, among medical specialties, anesthesiologists benefit from good feedback, because the effects of their actions are likely to be quickly evident. In contrast, radiologists, obtain little immediate information about the accuracy of their diagnoses.

Despite all of them might have acquired intuitive skills, the latter have failed to identify the situations and the tasks in which intuition will betray them.

Expertise is not a single skill, it is a collection of skills.

📍 Planning fallacy consists in making decisions based on delusional optimism rather than on a rational weighting of gains, losses, and probabilities.

- Plans and forecasts that are unrealistically close to best-case scenarios.
- Could be improved just by consulting statistics (outside view) of similar cases.

Statistical information — the outside view — is routinely discarded when it is incompatible with one's personal impressions of a case. In the competition with the inside view, the outside view doesn't stand a chance.

Yet incorporating all distributional information available about the class reference — from other ventures similar to that being forecasted (taking the outside view) is the cure to the planning fallacy.

Yet sometimes, when the outside view is presented in the midst of the project it is discarded because of the sunk-cost fallacy. It is easier to change directions on a crisis, but because this is not a crisis — just new facts have been presented.

> If you were allowed one wish for your child, consider wishing her optimism.

Optimists are normally cheerful and happy, and therefore popular; [...] their chances of depression are reduced, their immune system is stronger, they take better care of their health [...] and in fact, they are likely to live longer.

🖇 Related to [Twelve Rules For Life](/blog/2018/12-rules-for-life) and its first rule: stand up straight with your shoulders back.

The people who have the greatest influence in the life of others are likely to be optimistic and overconfident.

🔖 Usually optimists show a higher tendency to:

- Focus on their goal, anchor on their plan and neglect relevant base rates — planning fallacy.
- Focus on they want or can do, neglecting skills of others — competition neglect and above-average effect, with the immediate consequence of excess entry to the market.
- Both explaining the past and trying to predict the future, focusing on the casual role of skill and neglect the role of luck aka. illusion of control.
- Focus in what they know, and neglect what they don't know, which makes them overly confident.

Optimism is highly valued, socially and in the market; people and firms reward the providers of dangerously misleading information more than they reward truth-tellers.
